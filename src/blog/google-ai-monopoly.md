<h1>Google is massively ahead in the AI battle of 2024</h1>
<div class="center">2024-03-23</div>

## Background

At the time of writing, AI battle is heating up and OpenAI has disrupted Google for the first time
in their history. The Search-Goliath is at all-time low-popularity across the media and people are
cheering for David in this battle.

Despite the disruptions, it is very likely that Google will be the one who will win this battle.
This is because of the dynamics outside the ML technology itself.

## OpenAI and Microsoft's detachment: risk management

There are multiple reasons why Microsoft is not making OpenAI part of Microsoft (unlike what Google
did with DeepMind) - MS ultimately does not want to be *responsible* for the technology and the
generated contents.

Currently, nothing can mathematically prove that the output of the content will be harmless. Even
with rigorous testing, we lack the historic experience to predict mishaps.

This leads to the core the issue: when something goes wrong, an actual entity (not "AI") needs to be
responsible for the outcome. In order for the tool-provider (ie. someone who wants to monetize
their model) to detach themselves from responsibility, they need to set boundaries on what the tool
is meant to do and ensure the tool is not 'defective'.

If model creator heavily restricts the models' capabilities to mitigate risks, the model loses its
ability to solve nuanced and difficult problems. Even adding such limitations cannot guarantee that
the model will not misbehave. Most importantly, there are not enough past cases to predict what the
legal consequences might be when AI models misbehave, hence the risk is multiplied.

## AirCanada taking actual responsibility

[Air Canada had to honour the discount made up by the AI chat bot to the
user](https://www.washingtonpost.com/travel/2024/02/18/air-canada-airline-chatbot-ruling/). This
proves that humans *need* an entity to take responsibility of LLM's outcome. This was a
light-hearted case where nobody seriously injured, but what if it was much worse? What if an
innocent life was lost?

Microsoft obviously do not want this headache and is extremely careful approaching the industry.
They are invested, but clearly drawing the boundaries. Everything is proxied via OpenAI and branded
with non-traditional ways - "Copilot" (with emphasis on the brand that **you** are still the
pilot). I think this is a smart move that fulfills my personal risk tolerance.

## Google, the bold one

Google managed to put their Gemini behind [Samsung's
devices](https://blog.google/products/android/google-ai-samsung-galaxy-s24/) and at the time of
writing, soon [Apple's as
well](https://www.forbes.com/sites/siladityaray/2024/03/18/apple-reportedly-in-talks-with-google-to-integrate-geminis-ai-service-into-the-iphone).

The branding in Samsung's "Galaxy AI" is still using the Gemini logo, with the disclaimer "Samsung
does not make any promises, assurances or guarantees as to the accuracy, completeness or reliability
of the output provided by AI features.". Although I have no information, I can guarantee that Apple
will have a similar term.

However, in the current [Google AI Terms of Service](https://ai.google.dev/terms), it does
not explicitly deny responsibility of all generated content. However, it guards itself against
certain scenarios and industries which sounds more vulnerable:

- "You may not use the Services in clinical practice, to provide medical advice, or in any manner
  that is overseen by or requires clearance or approval from a medical device regulatory agency."
- "You may not attempt to bypass these protective measures or use content that violates the API
  Terms or these Additional Terms"
- "You may not use the Services in clinical practice, to provide medical advice, or in any manner
  that is overseen by or requires clearance or approval from a medical device regulatory agency."
- "Don't rely on the Services for medical, legal, financial, or other professional advice."
- "The Services use experimental technology and may sometimes provide inaccurate or offensive
  content that doesn't represent Google's views."

### Why is Google willing to take this risk?

It seems like Google is confident with its ability around model 'safety' during training. They may
also have established relationship with various local governments to mitigate risks. I also would
not be surprised if engineers from Google and partners behind the scenes are working very closely
together to limit the LLM applications to control the risk.

I have an uneducated opinion that if generated content is highly personalized, lawsuit/PR risk
reduces astronomically:

- In the event of generated content doing harm, if Google can prove that the generated content was
  highly tailored, it may have grounds to argue that outcome was self-inflicted by the user. This
  will hand over the responsibility to the user.
- If the model's "opinions" are tailored, then nobody can argue the model is politically biased.

I cannot name any other company that has enough user information and behavioural analysis to achieve
this.

## Video generation and YouTube

I think real generated content war will start with videos (nobody likes reading wall of text, ironic
for this blog).

[OpenAI Sora](https://openai.com/sora) is jaw-dropping (probably trained using Unreal Engine
renders), but the party trick of nice visuals is insufficient. Just like video games, fancy graphics
is not what keeps users hooked. It is the content inside it.

What will make users use video generation will be ability to generate good *content* - the formula
that YouTubers have to keep the audience engaged for a reasonable period of time for monetization.

This means if we want user-generated videos with very little (critical for UX) input, something in
the generation chain needs to understand what is makes a good and addictive video to watch for the
user.

Google already has monopoly in this industry with YouTube, and has access to incredible amounts of
video data that no other entity does. Google is at a massive head start in this AI race.

## What if Google has a monopoly, again

I think we, as consumers, want competition in the market and do not want mono/duopolies.

Ultimately, the first big winner of AI era would be the entity who can actually make sizable money
by using new tech, solving a problem that was not possible before.

There is a descent chance that Google's side (Samsung, Apple and Google itself) will be the first
to achieve highly profitable and sustainable AI product, it almost seems inevitable. Then Google
will have access to data and information on *what* makes a profitable model and the industry will
start revolving around this benchmark/fact - and the centre of this will be Google.

However, normal people and companies have something that G's family cannot execute in this race -
agility to adapt. The only way to prevent upcoming monopoly is to beat them in this race by creating
a successful product utilizing new tech first, then "we" get to define what makes a profitable
model.

In short, startups need to **stop** focusing on model pre-training, but instead, work on fine-tuning
and building products that will change the world with the new tech.
